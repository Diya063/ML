
1. NaÃ¯ve Bayes Classifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report

data = load_iris()
X = data.data
y = data.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = GaussianNB()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print(classification_report(y_test, y_pred))


2. Simple Linear Regression
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

X = np.random.rand(100, 1) * 10
y = 2.5 * X.flatten() + np.random.randn(100) * 2

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print("MSE:", mean_squared_error(y_test, y_pred))
print("RÂ² Score:", r2_score(y_test, y_pred))


3. Multiple Linear Regression
X = np.random.rand(100, 3)
y = 3*X[:,0] + 2*X[:,1] + X[:,2] + np.random.randn(100)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print("MSE:", mean_squared_error(y_test, y_pred))
print("RÂ² Score:", r2_score(y_test, y_pred))



4. Polynomial Regression
from sklearn.preprocessing import PolynomialFeatures

X = np.random.rand(100, 1) * 10
y = 1.2*X.flatten()**2 + 0.5*X.flatten() + np.random.randn(100)*5

poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print("MSE:", mean_squared_error(y_test, y_pred))
print("RÂ² Score:", r2_score(y_test, y_pred))




5. Lasso and Ridge Regression
from sklearn.linear_model import Lasso, Ridge

ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)
print("Ridge RÂ²:", ridge.score(X_test, y_test))

lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)
print("Lasso RÂ²:", lasso.score(X_test, y_test))



6. Logistic Regression
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_breast_cancer
from sklearn.metrics import classification_report

data = load_breast_cancer()
X = data.data
y = data.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LogisticRegression(max_iter=10000)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print(classification_report(y_test, y_pred))



7. Artificial Neural Network
from sklearn.neural_network import MLPClassifier

model = MLPClassifier(hidden_layer_sizes=(10,), max_iter=1000)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print(classification_report(y_test, y_pred))



8. K-Nearest Neighbors (KNN) Classifier
from sklearn.neighbors import KNeighborsClassifier

model = KNeighborsClassifier(n_neighbors=5)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print(classification_report(y_test, y_pred))



9. Decision Tree Classifier
from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print(classification_report(y_test, y_pred))



10. Support Vector Machine (SVM)
from sklearn.svm import SVC

model = SVC()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print(classification_report(y_test, y_pred))



11. K-Means Clustering
from sklearn.cluster import KMeans

model = KMeans(n_clusters=3, random_state=42)
model.fit(X)
labels = model.labels_
print("Cluster labels:", labels[:10])



12. Hierarchical Clustering
from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt

linked = linkage(X, 'single')
plt.figure(figsize=(10, 7))
dendrogram(linked)
plt.show()


# ðŸ“Œ Step 1: Import Required Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# ðŸ“Œ Step 2: Load the CSV File
df = pd.read_csv('data.csv')  # Replace with your filename
print("Dataset:")
print(df)

# ðŸ“Œ Step 3: Extract Features and Target
X = df[['Hours']].values
y = df['Scores'].values

# ðŸ“Œ Step 4: Normalize the Data (Min-Max)
X_min, X_max = X.min(), X.max()
y_min, y_max = y.min(), y.max()

X_norm = (X - X_min) / (X_max - X_min)
y_norm = (y - y_min) / (y_max - y_min)

# ðŸ“Œ Step 5: Add Bias Term (X0 = 1)
X_b = np.c_[np.ones((X_norm.shape[0], 1)), X_norm]

# ðŸ“Œ Step 6: Initialize Parameters
theta = np.zeros(2)
alpha = 0.01
epochs = 1000
m = len(X_b)

# ðŸ“Œ Step 7: Gradient Descent Without Regularization
for i in range(epochs):
    predictions = X_b.dot(theta)
    errors = predictions - y_norm
    gradients = (2/m) * X_b.T.dot(errors)
    theta -= alpha * gradients

print("\nðŸŽ¯ Parameters after Gradient Descent (No Regularization):", theta)

# ðŸ“Œ Step 8: Predict on Normalized Data
y_pred_norm = X_b.dot(theta)
y_pred = y_pred_norm * (y_max - y_min) + y_min

# ðŸ“Œ Step 9: Plot Predictions
plt.scatter(X, y, color='blue', label='Actual')
plt.plot(X, y_pred, color='red', label='Prediction')
plt.xlabel("Hours")
plt.ylabel("Scores")
plt.title("Linear Regression (Manual GD)")
plt.legend()
plt.show()

# ðŸ“Œ Step 10: Ridge Regression (L2)
theta_ridge = np.zeros(2)
lambda_ = 0.1

for i in range(epochs):
    predictions = X_b.dot(theta_ridge)
    errors = predictions - y_norm
    gradients = (2/m) * X_b.T.dot(errors) + 2 * lambda_ * theta_ridge
    gradients[0] -= 2 * lambda_ * theta_ridge[0]  # don't regularize bias
    theta_ridge -= alpha * gradients

print("\n Parameters after Ridge Regularization:", theta_ridge)

# ðŸ“Œ Step 11: Lasso Regression (L1)
theta_lasso = np.zeros(2)
lambda_ = 0.1

for i in range(epochs):
    predictions = X_b.dot(theta_lasso)
    errors = predictions - y_norm
    gradients = (2/m) * X_b.T.dot(errors) + lambda_ * np.sign(theta_lasso)
    gradients[0] -= lambda_ * np.sign(theta_lasso[0])
    theta_lasso -= alpha * gradients

print("\n Parameters after Lasso Regularization:", theta_lasso)

# Full implementation of 13 machine learning algorithms without using external libraries

# Dataset for regression models (Hours vs Scores)
X_reg = [1, 2, 3, 4, 5]
y_reg = [1.5, 3.2, 4.5, 6.4, 7.5]

# Dataset for classification models (feature: hours, label: pass/fail)
X_clf = [1, 2, 3, 4, 5]
y_clf = [0, 0, 0, 1, 1]

# 1. Simple Linear Regression
def simple_linear_regression(X, y):
    n = len(X)
    mean_x = sum(X) / n
    mean_y = sum(y) / n
    num = sum((X[i] - mean_x) * (y[i] - mean_y) for i in range(n))
    den = sum((X[i] - mean_x) ** 2 for i in range(n))
    b1 = num / den
    b0 = mean_y - b1 * mean_x
    return b0, b1

# 2. Multiple Linear Regression
def multiple_linear_regression(X, y):
    # For simplicity, assume X is a list of lists
    from copy import deepcopy
    def transpose(matrix):
        return list(map(list, zip(*matrix)))

    def matmul(A, B):
        return [[sum(a * b for a, b in zip(row, col)) for col in zip(*B)] for row in A]

    def inverse(matrix):
        # Only 2x2 inverse for simplicity
        a, b, c, d = matrix[0][0], matrix[0][1], matrix[1][0], matrix[1][1]
        det = a * d - b * c
        return [[d / det, -b / det], [-c / det, a / det]]

    X_b = [[1] + row for row in X]  # Add bias term
    XT = transpose(X_b)
    XTX = matmul(XT, X_b)
    XTX_inv = inverse(XTX)
    XTy = matmul(XT, [[yi] for yi in y])
    theta = matmul(XTX_inv, XTy)
    return [t[0] for t in theta]

# 3. Polynomial Regression (degree 2)
def polynomial_regression(X, y, degree=2):
    X_poly = [[x ** d for d in range(degree + 1)] for x in X]
    return multiple_linear_regression(X_poly, y)

# 4. Ridge Regression (with Gradient Descent)
def ridge_regression(X, y, alpha=0.01, lambda_=0.1, epochs=1000):
    theta = [0, 0]
    m = len(X)
    for _ in range(epochs):
        preds = [theta[0] + theta[1] * X[i] for i in range(m)]
        errors = [preds[i] - y[i] for i in range(m)]
        grad0 = (2 / m) * sum(errors)
        grad1 = (2 / m) * sum(errors[i] * X[i] for i in range(m)) + 2 * lambda_ * theta[1]
        theta[0] -= alpha * grad0
        theta[1] -= alpha * grad1
    return theta

# 5. Lasso Regression (with Gradient Descent)
def lasso_regression(X, y, alpha=0.01, lambda_=0.1, epochs=1000):
    theta = [0, 0]
    m = len(X)
    for _ in range(epochs):
        preds = [theta[0] + theta[1] * X[i] for i in range(m)]
        errors = [preds[i] - y[i] for i in range(m)]
        grad0 = (2 / m) * sum(errors)
        grad1 = (2 / m) * sum(errors[i] * X[i] for i in range(m)) + lambda_ * (1 if theta[1] > 0 else -1)
        theta[0] -= alpha * grad0
        theta[1] -= alpha * grad1
    return theta

# 6. Logistic Regression
def sigmoid(z):
    return 1 / (1 + pow(2.71828, -z))

def logistic_regression(X, y, alpha=0.1, epochs=1000):
    theta = [0, 0]
    m = len(X)
    for _ in range(epochs):
        for i in range(m):
            z = theta[0] + theta[1] * X[i]
            pred = sigmoid(z)
            error = pred - y[i]
            theta[0] -= alpha * error
            theta[1] -= alpha * error * X[i]
    return theta

# 7. K-Nearest Neighbors
def knn(X_train, y_train, x_test, k=3):
    distances = [(abs(x_test - X_train[i]), y_train[i]) for i in range(len(X_train))]
    distances.sort()
    top_k = [label for _, label in distances[:k]]
    return max(set(top_k), key=top_k.count)

# 8. Decision Tree (Stump for simplicity)
def decision_tree(X, y):
    threshold = sum(X) / len(X)
    left_label = max(set([y[i] for i in range(len(X)) if X[i] <= threshold]), key=y.count)
    right_label = max(set([y[i] for i in range(len(X)) if X[i] > threshold]), key=y.count)
    return threshold, left_label, right_label

# 9. Support Vector Machine (linear, no kernel, hard-margin)
def svm(X, y, alpha=0.01, epochs=1000):
    w, b = 0, 0
    for _ in range(epochs):
        for i in range(len(X)):
            condition = y[i] * (w * X[i] + b) >= 1
            if condition:
                w -= alpha * (2 * 1e-4 * w)
            else:
                w += alpha * (y[i] * X[i] - 2 * 1e-4 * w)
                b += alpha * y[i]
    return w, b

# 10. K-Means Clustering
def kmeans(X, k=2, epochs=10):
    centers = [X[0], X[-1]]
    for _ in range(epochs):
        clusters = [[] for _ in range(k)]
        for x in X:
            dists = [abs(x - c) for c in centers]
            clusters[dists.index(min(dists))].append(x)
        centers = [sum(cluster) / len(cluster) if cluster else c for cluster, c in zip(clusters, centers)]
    return centers, clusters

# 11. Hierarchical Clustering (Single Linkage)
def hierarchical_clustering(X):
    clusters = [[x] for x in X]
    while len(clusters) > 1:
        min_dist = float('inf')
        to_merge = (0, 1)
        for i in range(len(clusters)):
            for j in range(i + 1, len(clusters)):
                d = min(abs(a - b) for a in clusters[i] for b in clusters[j])
                if d < min_dist:
                    min_dist, to_merge = d, (i, j)
        i, j = to_merge
        clusters[i] += clusters[j]
        del clusters[j]
    return clusters

# 12. Neural Network (1 neuron, sigmoid)
def simple_neural_net(X, y, alpha=0.1, epochs=1000):
    w, b = 0.0, 0.0
    for _ in range(epochs):
        for i in range(len(X)):
            z = w * X[i] + b
            pred = sigmoid(z)
            error = pred - y[i]
            w -= alpha * error * X[i]
            b -= alpha * error
    return w, b

# 13. Gradient Descent (Generic Example)
def gradient_descent(X, y, alpha=0.01, epochs=1000):
    theta0, theta1 = 0, 0
    m = len(X)
    for _ in range(epochs):
        predictions = [theta0 + theta1 * X[i] for i in range(m)]
        errors = [predictions[i] - y[i] for i in range(m)]
        grad0 = (2 / m) * sum(errors)
        grad1 = (2 / m) * sum(errors[i] * X[i] for i in range(m))
        theta0 -= alpha * grad0
        theta1 -= alpha * grad1
    return theta0, theta1

# Output a summary of all algorithm results
results = {
    "Simple Linear Regression": simple_linear_regression(X_reg, y_reg),
    "Polynomial Regression": polynomial_regression(X_reg, y_reg),
    "Ridge Regression": ridge_regression(X_reg, y_reg),
    "Lasso Regression": lasso_regression(X_reg, y_reg),
    "Logistic Regression": logistic_regression(X_clf, y_clf),
    "KNN (test=3)": knn(X_clf, y_clf, 3),
    "Decision Tree": decision_tree(X_clf, y_clf),
    "SVM": svm(X_clf, [1 if y == 1 else -1 for y in y_clf]),
    "KMeans": kmeans(X_reg),
    "Hierarchical Clustering": hierarchical_clustering(X_reg),
    "Neural Net": simple_neural_net(X_clf, y_clf),
    "Gradient Descent": gradient_descent(X_reg, y_reg)
}

results

