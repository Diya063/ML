
1. NaÃ¯ve Bayes Classifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report

data = load_iris()
X = data.data
y = data.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = GaussianNB()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print(classification_report(y_test, y_pred))


2. Simple Linear Regression
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

X = np.random.rand(100, 1) * 10
y = 2.5 * X.flatten() + np.random.randn(100) * 2

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print("MSE:", mean_squared_error(y_test, y_pred))
print("RÂ² Score:", r2_score(y_test, y_pred))


3. Multiple Linear Regression
X = np.random.rand(100, 3)
y = 3*X[:,0] + 2*X[:,1] + X[:,2] + np.random.randn(100)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print("MSE:", mean_squared_error(y_test, y_pred))
print("RÂ² Score:", r2_score(y_test, y_pred))



4. Polynomial Regression
from sklearn.preprocessing import PolynomialFeatures

X = np.random.rand(100, 1) * 10
y = 1.2*X.flatten()**2 + 0.5*X.flatten() + np.random.randn(100)*5

poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print("MSE:", mean_squared_error(y_test, y_pred))
print("RÂ² Score:", r2_score(y_test, y_pred))




5. Lasso and Ridge Regression
from sklearn.linear_model import Lasso, Ridge

ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)
print("Ridge RÂ²:", ridge.score(X_test, y_test))

lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)
print("Lasso RÂ²:", lasso.score(X_test, y_test))



6. Logistic Regression
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_breast_cancer
from sklearn.metrics import classification_report

data = load_breast_cancer()
X = data.data
y = data.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LogisticRegression(max_iter=10000)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print(classification_report(y_test, y_pred))



7. Artificial Neural Network
from sklearn.neural_network import MLPClassifier

model = MLPClassifier(hidden_layer_sizes=(10,), max_iter=1000)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print(classification_report(y_test, y_pred))



8. K-Nearest Neighbors (KNN) Classifier
from sklearn.neighbors import KNeighborsClassifier

model = KNeighborsClassifier(n_neighbors=5)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print(classification_report(y_test, y_pred))



9. Decision Tree Classifier
from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print(classification_report(y_test, y_pred))



10. Support Vector Machine (SVM)
from sklearn.svm import SVC

model = SVC()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print(classification_report(y_test, y_pred))



11. K-Means Clustering
from sklearn.cluster import KMeans

model = KMeans(n_clusters=3, random_state=42)
model.fit(X)
labels = model.labels_
print("Cluster labels:", labels[:10])



12. Hierarchical Clustering
from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt

linked = linkage(X, 'single')
plt.figure(figsize=(10, 7))
dendrogram(linked)
plt.show()


# ðŸ“Œ Step 1: Import Required Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# ðŸ“Œ Step 2: Load the CSV File
df = pd.read_csv('data.csv')  # Replace with your filename
print("Dataset:")
print(df)

# ðŸ“Œ Step 3: Extract Features and Target
X = df[['Hours']].values
y = df['Scores'].values

# ðŸ“Œ Step 4: Normalize the Data (Min-Max)
X_min, X_max = X.min(), X.max()
y_min, y_max = y.min(), y.max()

X_norm = (X - X_min) / (X_max - X_min)
y_norm = (y - y_min) / (y_max - y_min)

# ðŸ“Œ Step 5: Add Bias Term (X0 = 1)
X_b = np.c_[np.ones((X_norm.shape[0], 1)), X_norm]

# ðŸ“Œ Step 6: Initialize Parameters
theta = np.zeros(2)
alpha = 0.01
epochs = 1000
m = len(X_b)

# ðŸ“Œ Step 7: Gradient Descent Without Regularization
for i in range(epochs):
    predictions = X_b.dot(theta)
    errors = predictions - y_norm
    gradients = (2/m) * X_b.T.dot(errors)
    theta -= alpha * gradients

print("\nðŸŽ¯ Parameters after Gradient Descent (No Regularization):", theta)

# ðŸ“Œ Step 8: Predict on Normalized Data
y_pred_norm = X_b.dot(theta)
y_pred = y_pred_norm * (y_max - y_min) + y_min

# ðŸ“Œ Step 9: Plot Predictions
plt.scatter(X, y, color='blue', label='Actual')
plt.plot(X, y_pred, color='red', label='Prediction')
plt.xlabel("Hours")
plt.ylabel("Scores")
plt.title("Linear Regression (Manual GD)")
plt.legend()
plt.show()

# ðŸ“Œ Step 10: Ridge Regression (L2)
theta_ridge = np.zeros(2)
lambda_ = 0.1

for i in range(epochs):
    predictions = X_b.dot(theta_ridge)
    errors = predictions - y_norm
    gradients = (2/m) * X_b.T.dot(errors) + 2 * lambda_ * theta_ridge
    gradients[0] -= 2 * lambda_ * theta_ridge[0]  # don't regularize bias
    theta_ridge -= alpha * gradients

print("\n Parameters after Ridge Regularization:", theta_ridge)

# ðŸ“Œ Step 11: Lasso Regression (L1)
theta_lasso = np.zeros(2)
lambda_ = 0.1

for i in range(epochs):
    predictions = X_b.dot(theta_lasso)
    errors = predictions - y_norm
    gradients = (2/m) * X_b.T.dot(errors) + lambda_ * np.sign(theta_lasso)
    gradients[0] -= lambda_ * np.sign(theta_lasso[0])
    theta_lasso -= alpha * gradients

print("\n Parameters after Lasso Regularization:", theta_lasso)

